<!doctype html><html lang=zh-Hans dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="ç‰ˆæœ¬æ‰€æœ‰ï¼šÂ© 2023 Reworkd AI, Inc. åŸæ–‡é“¾æ¥ï¼šReworkd Blog ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰çš„å‘æ˜æ˜¯è¿‘åå¹´æ¥äººå·¥æ™ºèƒ½æŠ€æœ¯æœ€é‡è¦çš„è¿›æ­¥ä¹‹ä¸€ã€‚ä¸ºå½“ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹"><title>æ´æ‚‰ Agent-GPT</title><link rel=canonical href=https://umpire2018.github.io/p/%E6%B4%9E%E6%82%89-agent-gpt/><link rel=stylesheet href=/scss/style.min.abbd69b2908fdfcd5179898beaafd374514a86538d81639ddd2c58c06ae54e40.css><meta property="og:title" content="æ´æ‚‰ Agent-GPT"><meta property="og:description" content="ç‰ˆæœ¬æ‰€æœ‰ï¼šÂ© 2023 Reworkd AI, Inc. åŸæ–‡é“¾æ¥ï¼šReworkd Blog ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰çš„å‘æ˜æ˜¯è¿‘åå¹´æ¥äººå·¥æ™ºèƒ½æŠ€æœ¯æœ€é‡è¦çš„è¿›æ­¥ä¹‹ä¸€ã€‚ä¸ºå½“ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹"><meta property="og:url" content="https://umpire2018.github.io/p/%E6%B4%9E%E6%82%89-agent-gpt/"><meta property="og:site_name" content="Arno's Blog"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:published_time" content="2023-09-21T20:07:13+08:00"><meta property="article:modified_time" content="2023-09-21T20:07:13+08:00"><meta name=twitter:title content="æ´æ‚‰ Agent-GPT"><meta name=twitter:description content="ç‰ˆæœ¬æ‰€æœ‰ï¼šÂ© 2023 Reworkd AI, Inc. åŸæ–‡é“¾æ¥ï¼šReworkd Blog ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰çš„å‘æ˜æ˜¯è¿‘åå¹´æ¥äººå·¥æ™ºèƒ½æŠ€æœ¯æœ€é‡è¦çš„è¿›æ­¥ä¹‹ä¸€ã€‚ä¸ºå½“ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu648d6f62cb85fc48f15853bd9bc5009b_4973957_300x0_resize_box_3.png width=300 height=240 class=site-logo loading=lazy alt=Avatar></a>
<span class=emoji>ğŸ˜</span></figure><div class=site-meta><h1 class=site-name><a href=/>Arno's Blog</a></h1><h2 class=site-description>æ‚Ÿå·²å¾€è€…ä¸è°,çŸ¥æ¥è€…çŸ¥å¯è¿½</h2></div></header><ol class=social-menu><li><a href=https://github.com/Umpire2018 target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li class=current><a href=/post/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg><span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg><span>Links</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>æš—è‰²æ¨¡å¼</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#llm-æœ‰å¾ˆå¤šé™åˆ¶>LLM æœ‰å¾ˆå¤šé™åˆ¶</a></li><li><a href=#what-are-agentsä»€ä¹ˆæ˜¯ä»£ç†>What are agents?Â ä»€ä¹ˆæ˜¯ä»£ç†ï¼Ÿ</a><ol><li><a href=#engineering-this-system-consists-of-3-partsè¯¥ç³»ç»Ÿçš„å·¥ç¨‹ç”±-3-éƒ¨åˆ†ç»„æˆ>Engineering this system consists of 3 parts.è¯¥ç³»ç»Ÿçš„å·¥ç¨‹ç”± 3 éƒ¨åˆ†ç»„æˆã€‚</a></li></ol></li><li><a href=#how-do-we-get-agents-to-act-intelligentlyæˆ‘ä»¬å¦‚ä½•è®©ä»£ç†äººæ˜æ™ºåœ°è¡ŒåŠ¨>How do we get agents to act intelligently?æˆ‘ä»¬å¦‚ä½•è®©ä»£ç†äººæ˜æ™ºåœ°è¡ŒåŠ¨ï¼Ÿ</a><ol><li><a href=#a-brief-intro-to-prompt-engineeringå¿«é€Ÿå·¥ç¨‹ç®€ä»‹>A Brief Intro to Prompt Engineeringå¿«é€Ÿå·¥ç¨‹ç®€ä»‹</a></li><li><a href=#how-agentgpt-uses-prompt-engineeringagentgpt-å¦‚ä½•ä½¿ç”¨å³æ—¶å·¥ç¨‹>How AgentGPT Uses Prompt EngineeringAgentGPT å¦‚ä½•ä½¿ç”¨å³æ—¶å·¥ç¨‹</a></li></ol></li><li><a href=#how-do-we-give-agents-a-working-memoryæˆ‘ä»¬å¦‚ä½•ä¸ºä»£ç†æä¾›å·¥ä½œè®°å¿†>How do we give agents a working memory?æˆ‘ä»¬å¦‚ä½•ä¸ºä»£ç†æä¾›å·¥ä½œè®°å¿†ï¼Ÿ</a><ol><li><a href=#vector-databases-demystifiedæ­ç§˜çŸ¢é‡æ•°æ®åº“>Vector Databases Demystifiedæ­ç§˜çŸ¢é‡æ•°æ®åº“</a></li><li><a href=#vector-libraries-likeå‘é‡åº“å¦‚>Vector libraries likeÂ å‘é‡åº“å¦‚</a></li></ol></li><li><a href=#tools-to-interact-with-the-environmentä¸ç¯å¢ƒäº¤äº’çš„å·¥å…·>Tools to interact with the environmentä¸ç¯å¢ƒäº¤äº’çš„å·¥å…·</a><ol><li><a href=#engineering-robust-function-callså·¥ç¨‹ç¨³å¥çš„å‡½æ•°è°ƒç”¨>Engineering Robust Function Callså·¥ç¨‹ç¨³å¥çš„å‡½æ•°è°ƒç”¨</a></li></ol></li><li><a href=#the-future-of-llm-powered-agents-is-brightllm-ä»£ç†äººçš„æœªæ¥æ˜¯å…‰æ˜çš„>The future of LLM-powered agents is bright!LLM ä»£ç†äººçš„æœªæ¥æ˜¯å…‰æ˜çš„ï¼</a></li><li><a href=#conclusionç»“è®º>ConclusionÂ ç»“è®º</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/ai/>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E6%B4%9E%E6%82%89-agent-gpt/>æ´æ‚‰ Agent-GPT</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Sep 21, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>é˜…è¯»æ—¶é•¿: 16 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><blockquote><p>ç‰ˆæœ¬æ‰€æœ‰ï¼šÂ© 2023 Reworkd AI, Inc.</p><p>åŸæ–‡é“¾æ¥ï¼š<a class=link href=https://reworkd.ai/blog/Understanding-AgentGPT target=_blank rel=noopener>Reworkd Blog</a></p></blockquote><p>ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰çš„å‘æ˜æ˜¯è¿‘åå¹´æ¥äººå·¥æ™ºèƒ½æŠ€æœ¯æœ€é‡è¦çš„è¿›æ­¥ä¹‹ä¸€ã€‚ä¸ºå½“ä»Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æä¾›æ”¯æŒçš„ GPT å±•ç¤ºäº†å“è¶Šçš„æ¨ç†ã€ç†è§£å’Œè§„åˆ’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„çœŸæ­£æ½œåŠ›å°šæœªå®Œå…¨å‘æŒ¥ã€‚</p><p>åœ¨ Reworkdï¼Œæˆ‘ä»¬ç›¸ä¿¡ LLMs çš„çœŸæ­£åŠ›é‡åœ¨äºä»£ç†è¡Œä¸ºã€‚é€šè¿‡è®¾è®¡ä¸€ä¸ªåˆ©ç”¨ LLM çš„æ–°å…´èƒ½åŠ›çš„ç³»ç»Ÿå¹¶æä¾›æ”¯æŒç¯å¢ƒç›¸äº’ä½œç”¨çš„ç”Ÿæ€ç³»ç»Ÿï¼Œæˆ‘ä»¬å¯ä»¥å……åˆ†å‘æŒ¥ GPT-4 ç­‰æ¨¡å‹çš„æ½œåŠ›ã€‚ä»¥ä¸‹æ˜¯ AgentGPT çš„å·¥ä½œåŸç†ã€‚</p><h2 id=llm-æœ‰å¾ˆå¤šé™åˆ¶>LLM æœ‰å¾ˆå¤šé™åˆ¶</h2><p><a class=link href=https://www.techopedia.com/definition/34826/foundation-model target=_blank rel=noopener>Foundation Model - Techopedia</a>. åŸºç¡€æ¨¡å‹ - æŠ€æœ¯ç™¾ç§‘ã€‚</p><p>å¦‚æœæ‚¨ç†Ÿæ‚‰ OpenAI çš„ APIï¼Œåˆ™ä¸æ¨¡å‹èŠå¤©æ—¶å¯èƒ½ä½¿ç”¨çš„å¸¸ç”¨å…¬å¼å¯èƒ½åŒ…æ‹¬ï¼š</p><ul><li>è·å–ç”¨æˆ·æŒ‡ä»¤ã€‚</li><li>æ·»åŠ èŠå¤©å†å²ã€‚</li><li>é€šè¿‡ API å‘é€èŠå¤©å†å²è®°å½•ä»¥è·å–å›ç­”ã€‚</li></ul><p>å½“å¯¹è¯èŒƒå›´è¾ƒå°æ—¶ï¼Œæ­¤æ–¹æ³•æ•ˆæœå¾ˆå¥½ï¼›ç„¶è€Œï¼Œå½“æ‚¨ç»§ç»­å‘èŠå¤©å†å²è®°å½•ä¸­æ·»åŠ æ–°æ¶ˆæ¯æ—¶ï¼Œå®Œæˆçš„å¤§å°å’Œå¤æ‚æ€§ä¼šä¸æ–­å¢åŠ ï¼Œæ‚¨å¾ˆå¿«å°±ä¼šç¢°å£ï¼šå¯æ€•çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚</p><p>é¦–å…ˆå¼•å…¥ <strong>tokens</strong> è¿™ä¸ªæ¦‚å¿µï¼Œå½¢å¦‚ å­— word æ˜¯äººç±»æ¥è®¡ç®—æ–‡æœ¬é•¿åº¦çš„æœ€å°å•ä½ï¼ˆæ¯”å¦‚ä¸€å¥è¯æœ‰åä¸ªå­—ï¼‰ï¼Œè€Œ tokens åˆ™æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æ–‡æœ¬é•¿åº¦çš„æœ€å°å•ä½ã€‚æ•°æ®è¡¨æ˜ï¼Œä¸€ä¸ªæ±‰å­—å¹³å‡åœ¨ ChatGPT(GPT3.5å’ŒGPT4) ä¸‹å¤§æ¦‚æ¶ˆè€—1.12ä¸ªtokenã€‚è€Œåœ¨ä¸€æ¬¡ä¸ Model è¿›è¡Œäº¤äº’çš„è¿‡ç¨‹ä¸­ï¼Œä¸Šä¸‹æ–‡é™åˆ¶åˆ™æ˜¯åœ¨ä¸€æ¬¡äº¤äº’è¿‡ç¨‹ä¸­å¯ä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„æœ€å¤§æ ‡è®°æ•°ã€‚éšç€æˆ‘ä»¬æ·»åŠ æ›´å¤šçš„ tokens ï¼ŒModel ç”¨äºè®¡ç®—çš„æˆæœ¬å¾€å¾€å‘ˆæŒ‡æ•°å¢é•¿ï¼Œæ‰€ä»¥è¿™å¾€å¾€æ˜¯æç¤ºå·¥ç¨‹å¸ˆçš„å­½ç¼˜ã€‚</p><p>ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ç»Ÿè®¡èŠå¤©å†å²è®°å½•ä¸­çš„ token æ•°é‡ï¼Œå°†æ—§æ¶ˆæ¯åˆ é™¤å†å°†å…¶å‘é€åˆ° Model ä¸­ä»¥ç¡®ä¿å…¶ç¬¦åˆ token é™åˆ¶ã€‚è™½ç„¶è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†å®ƒæœ€ç»ˆä¼šå‡å°‘å¯ç”¨äºæ±‚çŸ¥çš„çŸ¥è¯†é‡ã€‚</p><p>LLM ä»¬é¢ä¸´çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯éœ€è¦äººå·¥æŒ‡å¯¼ã€‚ä»æ ¹æœ¬ä¸Šè¯´ï¼ŒLLM ä»¬ç”¨æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„ï¼Œé€šå¸¸ï¼Œå®ƒä»¬çš„å†…éƒ¨ç»“æ„æœ¬è´¨ä¸Šå¹¶ä¸é€‚åˆé«˜é˜¶æ€ç»´è¿‡ç¨‹ï¼Œä¾‹å¦‚é€šè¿‡å¤æ‚ä»»åŠ¡è¿›è¡Œæ¨ç†ã€‚ä½†è¿™ç§ä¸è¶³å¹¶ä¸æ„å‘³ç€å®ƒä»¬ä¸èƒ½æˆ–ä¸ä¼š<strong>æ¨ç†</strong>ã€‚äº‹å®ä¸Šï¼Œæœ‰å‡ é¡¹ <a class=link href=https://arxiv.org/abs/2205.11916 target=_blank rel=noopener>ç ”ç©¶</a> è¡¨æ˜å®ƒä»¬å¯ä»¥ã€‚ç„¶è€Œï¼Œè¿™ç¡®å®æ„å‘³ç€å®ƒä»¬é¢ä¸´æŸäº›éšœç¢ã€‚ä¾‹å¦‚ï¼ŒLLM è‡ªèº«å¯ä»¥åˆ›å»ºä¸€ä¸ªé€»è¾‘æ­¥éª¤åˆ—è¡¨ï¼›ç„¶è€Œï¼Œå®ƒæ²¡æœ‰è§‚å¯Ÿå’Œåæ€è¯¥æ¸…å•çš„å†…ç½®æœºåˆ¶ã€‚</p><p>A pre-trained model is essentially a &ldquo;black box&rdquo; for the end user in which the final product that is shipped has <em>limited to no capability of actively updating its knowledge base and tends to act in unpredictable ways</em>. As a result, it&rsquo;s <a class=link href=https://arxiv.org/abs/2202.03629 target=_blank rel=noopener>hallucination</a>-prone.å¯¹äºæœ€ç»ˆç”¨æˆ·æ¥è¯´ï¼Œé¢„è®­ç»ƒæ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªâ€œé»‘åŒ£å­â€ï¼Œå…¶æœ€ç»ˆäº¤ä»˜çš„äº§å“å‡ ä¹æ²¡æœ‰ä¸»åŠ¨æ›´æ–°å…¶çŸ¥è¯†åº“çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å¾€å¾€ä»¥ä¸å¯é¢„æµ‹çš„æ–¹å¼è¿è¡Œï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“äº§ç”Ÿ<a class=link href=https://arxiv.org/abs/2202.03629 target=_blank rel=noopener>å¹»è§‰</a>ã€‚</p><p>Thus, it requires a lot of effort on the user&rsquo;s part to guide the model&rsquo;s output, and prompting the LLM itself becomes a job on its own. This extra work is a far cry from our vision of an AI-powered future.å› æ­¤ï¼Œéœ€è¦ç”¨æˆ·ä»˜å‡ºå¾ˆå¤§çš„åŠªåŠ›æ¥æŒ‡å¯¼æ¨¡å‹çš„è¾“å‡ºï¼Œè€Œä¿ƒä½¿LLMæœ¬èº«å°±æˆä¸ºäº†ä¸€é¡¹å·¥ä½œã€‚è¿™é¡¹é¢å¤–çš„å·¥ä½œä¸æˆ‘ä»¬å¯¹äººå·¥æ™ºèƒ½é©±åŠ¨çš„æœªæ¥çš„æ„¿æ™¯ç›¸å»ç”šè¿œã€‚</p><p>By providing a platform to give LLMs agentic abilities, <em>AgentGPT aims to overcome the limitations of standalone LLMs by leveraging prompt engineering techniques, vector databases, and API tooling.</em> Hereâ€™s some interesting work that is being done with the agent concept:é€šè¿‡æä¾›ä¸€ä¸ªå¹³å°æ¥èµ‹äºˆæ³•å­¦ç¡•å£«ä»£ç†èƒ½åŠ›ï¼ŒAgentGPT æ—¨åœ¨åˆ©ç”¨å³æ—¶å·¥ç¨‹æŠ€æœ¯ã€çŸ¢é‡æ•°æ®åº“å’Œ API å·¥å…·æ¥å…‹æœç‹¬ç«‹æ³•å­¦ç¡•å£«çš„å±€é™æ€§ã€‚ä»¥ä¸‹æ˜¯åˆ©ç”¨ä»£ç†æ¦‚å¿µæ‰€åšçš„ä¸€äº›æœ‰è¶£çš„å·¥ä½œï¼š</p><p><a class=link href=https://twitter.com/DrJimFan/status/1673006745067847683 target=_blank rel=noopener><img src="https://platform.twitter.com/embed/Tweet.html?dnt=false&amp;embedId=twitter-widget-0&amp;frame=false&amp;hideCard=false&amp;hideThread=false&amp;id=1673006745067847683&amp;lang=en&amp;origin=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps3A2F2Ftwitter.com2FDrJimFan2Fstatus2F1673006745067847683%26widget%3DTweet&amp;theme=light&amp;widgetsVersion=82e1070%3A1619632193066&amp;width=550px" loading=lazy alt="Tweet by Dr. Jim Fan"></a></p><blockquote><p>Alt: A Twitter post by Dr. Jim FanAltï¼šJim Fan åšå£«çš„ Twitter å¸–å­</p></blockquote><h2 id=what-are-agentsä»€ä¹ˆæ˜¯ä»£ç†>What are agents?Â ä»€ä¹ˆæ˜¯ä»£ç†ï¼Ÿ</h2><p>In a general sense, <a class=link href=https://zapier.com/blog/ai-agent/ target=_blank rel=noopener>agents</a> are rational actors. They use thinking and reasoning to influence their environment. <em>This could be in the form of solving problems or pursuing specific goals. They might interact with humans or utilize tools.</em> Ultimately, we can apply this concept to LLMs to instill more intelligent and logical behavior.
ä¸€èˆ¬æ„ä¹‰ä¸Šï¼Œä»£ç†äººæ˜¯ç†æ€§çš„è¡Œä¸ºè€…ã€‚ä»–ä»¬åˆ©ç”¨æ€è€ƒå’Œæ¨ç†æ¥å½±å“ä»–ä»¬çš„ç¯å¢ƒã€‚è¿™å¯ä»¥æ˜¯è§£å†³é—®é¢˜æˆ–è¿½æ±‚ç‰¹å®šç›®æ ‡çš„å½¢å¼ã€‚ä»–ä»¬å¯èƒ½ä¸äººç±»äº’åŠ¨æˆ–ä½¿ç”¨å·¥å…·ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸€æ¦‚å¿µåº”ç”¨äºæ³•å­¦ç¡•å£«ï¼Œä»¥çŒè¾“æ›´å¤šæ™ºèƒ½å’Œé€»è¾‘è¡Œä¸ºã€‚</p><p>In AgentGPT, large language models essentially function as the <strong>brain</strong> of each agent. As a result, we can produce powerful agents by cleverly <em>manipulating the English language</em> and engineering a <em>framework that supports interoperability between LLM completions and a diverse set of APIs</em>.
åœ¨ AgentGPT ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹æœ¬è´¨ä¸Šå……å½“æ¯ä¸ªä»£ç†çš„å¤§è„‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å·§å¦™åœ°æ“ä½œè‹±è¯­å¹¶è®¾è®¡ä¸€ä¸ªæ”¯æŒ LLM å®Œæˆå’Œå„ç§ API ä¹‹é—´äº’æ“ä½œæ€§çš„æ¡†æ¶æ¥ç”Ÿäº§å¼ºå¤§çš„ä»£ç†ã€‚</p><h3 id=engineering-this-system-consists-of-3-partsè¯¥ç³»ç»Ÿçš„å·¥ç¨‹ç”±-3-éƒ¨åˆ†ç»„æˆ>Engineering this system consists of 3 parts.è¯¥ç³»ç»Ÿçš„å·¥ç¨‹ç”± 3 éƒ¨åˆ†ç»„æˆã€‚</h3><p><strong>Reasoning and Planning.</strong> If you were to simply take a general goal, such as &ldquo;build a scaling e-commerce platform,&rdquo; and give it to ChatGPT, you would likely get a response along the lines of &ldquo;As an AI language modelâ€¦.&rdquo; However, through <strong>prompt engineering</strong>, we can get a model to <em>break down goals into digestible steps and reflect on them</em> with a method called chain of thought prompting.
æ¨ç†å’Œè®¡åˆ’ã€‚å¦‚æœæ‚¨åªæ˜¯ç®€å•åœ°æå‡ºä¸€ä¸ªæ€»ä½“ç›®æ ‡ï¼Œä¾‹å¦‚â€œæ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„ç”µå­å•†åŠ¡å¹³å°â€ï¼Œå¹¶å°†å…¶äº¤ç»™ ChatGPTï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°ç±»ä¼¼â€œä½œä¸ºäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹&mldr;&mldr;â€çš„å“åº”ã€‚ç„¶è€Œï¼Œé€šè¿‡æç¤ºå·¥ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå°†ç›®æ ‡åˆ†è§£ä¸ºå¯æ¶ˆåŒ–çš„æ­¥éª¤ï¼Œå¹¶ç”¨ä¸€ç§ç§°ä¸ºæ€ç»´é“¾æç¤ºçš„æ–¹æ³•å¯¹å…¶è¿›è¡Œåæ€ã€‚</p><p><strong>Memory.</strong> When dealing with memory, we divide the problem into <strong>short-term</strong> and <strong>long-term</strong>. In managing short-term memory, we can use prompting techniques such as <em>few-shot prompting to steer LLM responses</em>. However, <em>cost and context limits make it tricky to generate completions without limiting the breadth of information</em> a model can use to make decisions.
è®°å¿†ã€‚åœ¨å¤„ç†è®°å¿†æ—¶ï¼Œæˆ‘ä»¬å°†é—®é¢˜åˆ†ä¸ºçŸ­æœŸå’Œé•¿æœŸã€‚åœ¨ç®¡ç†çŸ­æœŸè®°å¿†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æç¤ºæŠ€æœ¯ï¼ˆä¾‹å¦‚å‡ æ¬¡æç¤ºï¼‰æ¥å¼•å¯¼æ³•å­¦ç¡•å£«çš„ååº”ã€‚ç„¶è€Œï¼Œæˆæœ¬å’Œä¸Šä¸‹æ–‡é™åˆ¶ä½¿å¾—åœ¨ä¸é™åˆ¶æ¨¡å‹å¯ç”¨äºåšå‡ºå†³ç­–çš„ä¿¡æ¯å¹¿åº¦çš„æƒ…å†µä¸‹ç”Ÿæˆå®Œæˆç»“æœå˜å¾—å¾ˆæ£˜æ‰‹ã€‚</p><p>Similarly, this issue also arises in <strong>long-term memory</strong> because it would be impossible to provide an appropriate corpus of writing to bridge the gap between GPT -4&rsquo;s cutoff date, 2021, till today. By using vector databases, we attempt to overcome this using specialized models for <em>information retrieval in high-dimensional vector spaces</em>.
åŒæ ·ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿå‡ºç°åœ¨é•¿æœŸè®°å¿†ä¸­ï¼Œå› ä¸ºä¸å¯èƒ½æä¾›é€‚å½“çš„å†™ä½œè¯­æ–™åº“æ¥å¼¥åˆ GPT -4 çš„æˆªæ­¢æ—¥æœŸ 2021 å¹´ä¸ä»Šå¤©ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡ä½¿ç”¨å‘é‡æ•°æ®åº“ï¼Œæˆ‘ä»¬å°è¯•ä½¿ç”¨é«˜ç»´å‘é‡ç©ºé—´ä¸­çš„ä¿¡æ¯æ£€ç´¢ä¸“ç”¨æ¨¡å‹æ¥å…‹æœè¿™ä¸ªé—®é¢˜ã€‚</p><p><strong>Tools</strong>. Another challenge in using LLMs as general actors is their confinement to text outputs. Again, we can use prompt engineering techniques to solve this issue. We can generate predictable function calls from the LLM through few-shot and chain-of-thought methods, utilizing API tools like <strong>Google Search</strong>, <strong>Hugging Face</strong>, <strong>Dall-E</strong>, etc. In addition, we can use fine-tuned LLMs that only return responses in specialized formatting, like JSON. This is the approach OpenAI took when they recently released the function calling feature for their API.
å·¥å…·ã€‚ä½¿ç”¨æ³•å­¦ç¡•å£«ä½œä¸ºä¸€èˆ¬å‚ä¸è€…çš„å¦ä¸€ä¸ªæŒ‘æˆ˜æ˜¯å®ƒä»¬ä»…é™äºæ–‡æœ¬è¾“å‡ºã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŠæ—¶çš„å·¥ç¨‹æŠ€æœ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ Google Searchã€Hugging Faceã€Dall-E ç­‰ API å·¥å…·ï¼Œé€šè¿‡å°‘æ ·æœ¬å’Œæ€ç»´é“¾æ–¹æ³•ä» LLM ç”Ÿæˆå¯é¢„æµ‹çš„å‡½æ•°è°ƒç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»…è¿”å›çš„å¾®è°ƒ LLMé‡‡ç”¨ç‰¹æ®Šæ ¼å¼çš„å“åº”ï¼Œä¾‹å¦‚ JSONã€‚è¿™æ˜¯ OpenAI æœ€è¿‘å‘å¸ƒå…¶ API çš„å‡½æ•°è°ƒç”¨åŠŸèƒ½æ—¶æ‰€é‡‡ç”¨çš„æ–¹æ³•ã€‚</p><p>These three concepts have formed the backbone of multiple successful agent-based LLM platforms such as <a class=link href=https://github.com/microsoft/JARVIS target=_blank rel=noopener>Microsoft Jarvis</a>, <a class=link href=https://github.com/Significant-Gravitas/Auto-GPT target=_blank rel=noopener>AutoGPT</a>, <a class=link href=https://github.com/yoheinakajima/babyagi target=_blank rel=noopener>BabyAGI</a>, and of course, AgentGPT. With this brief overview in mind, let&rsquo;s dive deeper into each component.
è¿™ä¸‰ä¸ªæ¦‚å¿µæ„æˆäº†å¤šä¸ªæˆåŠŸçš„åŸºäºä»£ç†çš„ LLM å¹³å°çš„æ”¯æŸ±ï¼Œä¾‹å¦‚ Microsoft Jarvisã€AutoGPTã€BabyAGIï¼Œå½“ç„¶è¿˜æœ‰ AgentGPTã€‚è®°ä½è¿™ä¸ªç®€çŸ­çš„æ¦‚è¿°ï¼Œè®©æˆ‘ä»¬æ›´æ·±å…¥åœ°äº†è§£æ¯ä¸ªç»„ä»¶ã€‚</p><h2 id=how-do-we-get-agents-to-act-intelligentlyæˆ‘ä»¬å¦‚ä½•è®©ä»£ç†äººæ˜æ™ºåœ°è¡ŒåŠ¨>How do we get agents to act intelligently?æˆ‘ä»¬å¦‚ä½•è®©ä»£ç†äººæ˜æ™ºåœ°è¡ŒåŠ¨ï¼Ÿ</h2><p><strong>Prompt engineering</strong> has become highly popularized, and it&rsquo;s only natural given its ability to <em>increase the reliability of LLM responses</em>, opening a wide avenue of potential applications for generative AI. AgentGPT&rsquo;s ability to think and reason is a result of novel prompting methods.
å³æ—¶å·¥ç¨‹å·²ç»é«˜åº¦æ™®åŠï¼Œè€ƒè™‘åˆ°å®ƒèƒ½å¤Ÿæé«˜æ³•å­¦ç¡•å£«ååº”çš„å¯é æ€§ï¼Œä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æ½œåœ¨åº”ç”¨å¼€è¾Ÿäº†å¹¿é˜”çš„é€”å¾„ï¼Œè¿™æ˜¯å¾ˆè‡ªç„¶çš„ã€‚ AgentGPT çš„æ€è€ƒå’Œæ¨ç†èƒ½åŠ›æ˜¯æ–°é¢–çš„æç¤ºæ–¹æ³•çš„ç»“æœã€‚</p><h3 id=a-brief-intro-to-prompt-engineeringå¿«é€Ÿå·¥ç¨‹ç®€ä»‹>A Brief Intro to Prompt Engineeringå¿«é€Ÿå·¥ç¨‹ç®€ä»‹</h3><p>Prompt engineering is a largely empirical field that aims to find methods to steer LLM responses by finding clever ways to use the English language. *You can think of it like lawyering, where every nuance in the wording of a prompt counts.
å³æ—¶å·¥ç¨‹æ˜¯ä¸€ä¸ªå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç»éªŒæ€§çš„é¢†åŸŸï¼Œæ—¨åœ¨é€šè¿‡å·§å¦™åœ°ä½¿ç”¨è‹±è¯­æ¥æ‰¾åˆ°æŒ‡å¯¼ LLM ååº”çš„æ–¹æ³•ã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆå¾‹å¸ˆï¼Œæç¤ºæªè¾ä¸­çš„æ¯ä¸€ä¸ªç»†å¾®å·®åˆ«éƒ½å¾ˆé‡è¦ã€‚</p><p>These are the main concepts and building blocks for more advanced prompting techniques:è¿™äº›æ˜¯æ›´é«˜çº§æç¤ºæŠ€æœ¯çš„ä¸»è¦æ¦‚å¿µå’Œæ„å»ºæ¨¡å—ï¼š</p><ol><li><strong>Zero-Shot</strong> involves sending the raw command directly to the LLM with little to no formatting.é›¶å°„å‡»æ¶‰åŠå°†åŸå§‹å‘½ä»¤ç›´æ¥å‘é€åˆ° LLMï¼Œå‡ ä¹ä¸éœ€è¦æ ¼å¼åŒ–ã€‚</li><li><strong>Few-Shot</strong> gives context for completions in the form of example responses.Few-Shot ä»¥ç¤ºä¾‹å“åº”çš„å½¢å¼æä¾›å®Œæˆçš„ä¸Šä¸‹æ–‡ã€‚</li><li><strong>Chain-of-Thought</strong> guides the model in reasoning through generating and reasoning over a complex task.æ€æƒ³é“¾é€šè¿‡å¯¹å¤æ‚ä»»åŠ¡çš„ç”Ÿæˆå’Œæ¨ç†æ¥æŒ‡å¯¼æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚</li></ol><h3 id=how-agentgpt-uses-prompt-engineeringagentgpt-å¦‚ä½•ä½¿ç”¨å³æ—¶å·¥ç¨‹>How AgentGPT Uses Prompt EngineeringAgentGPT å¦‚ä½•ä½¿ç”¨å³æ—¶å·¥ç¨‹</h3><p>AgentGPT uses an advanced form of chain-of-thought prompting called <strong>Plan-and-Solve</strong> to generate the steps you see when operating the agents.AgentGPT ä½¿ç”¨ä¸€ç§ç§°ä¸ºâ€œè®¡åˆ’ä¸è§£å†³â€çš„é«˜çº§æ€ç»´é“¾æç¤ºå½¢å¼æ¥ç”Ÿæˆæ‚¨åœ¨æ“ä½œä»£ç†æ—¶çœ‹åˆ°çš„æ­¥éª¤ã€‚</p><p>Traditionally, chain-of-thought prompting utilized few-shot techniques to provide examples of a thinking and reasoning process. However, as is becomes a theme, it becomes more costly as the complexity of a task increases because we will need to provide more context.
ä¼ ç»Ÿä¸Šï¼Œæ€ç»´é“¾æç¤ºåˆ©ç”¨å°æ ·æœ¬æŠ€æœ¯æ¥æä¾›æ€ç»´å’Œæ¨ç†è¿‡ç¨‹çš„ç¤ºä¾‹ã€‚ç„¶è€Œï¼Œå½“å®ƒæˆä¸ºä¸€ä¸ªä¸»é¢˜æ—¶ï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œå®ƒçš„æˆæœ¬ä¹Ÿä¼šå˜å¾—æ›´é«˜ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚</p><p><strong>Plan-and-solve (PS):</strong> By virtue of being a zero-shot method, it provides a <em>prompting framework for LLM-guided reasoning using &ldquo;trigger&rdquo; words</em>. These keywords trigger a reasoning response from the model.è®¡åˆ’ä¸è§£å†³ï¼ˆPSï¼‰ï¼šç”±äºæ˜¯ä¸€ç§é›¶æ ·æœ¬æ–¹æ³•ï¼Œå®ƒä¸ºä½¿ç”¨â€œè§¦å‘â€è¯çš„ LLM å¼•å¯¼æ¨ç†æä¾›äº†ä¸€ä¸ªæç¤ºæ¡†æ¶ã€‚è¿™äº›å…³é”®å­—è§¦å‘æ¨¡å‹çš„æ¨ç†å“åº”ã€‚</p><p>We can expand on this concept by <em>modifying the prompt to extract important variables and steps to generate a final response with a cohesive format</em>. This method allows us to parse the final response and display it for the end user as well as feed sub-steps into future plan-and-solve prompts.æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¿®æ”¹æå–é‡è¦å˜é‡çš„æç¤ºå’Œç”Ÿæˆå…·æœ‰å†…èšæ ¼å¼çš„æœ€ç»ˆå“åº”çš„æ­¥éª¤æ¥æ‰©å±•è¿™ä¸ªæ¦‚å¿µã€‚æ­¤æ–¹æ³•å…è®¸æˆ‘ä»¬è§£ææœ€ç»ˆå“åº”å¹¶å°†å…¶æ˜¾ç¤ºç»™æœ€ç»ˆç”¨æˆ·ï¼Œå¹¶å°†å­æ­¥éª¤æä¾›ç»™æœªæ¥çš„è®¡åˆ’å’Œè§£å†³æç¤ºã€‚</p><p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F29d8c98c-21e6-4991-992d-62d95fd40dba%2FScreen_Shot_2023-07-01_at_12.25.37_PM.png?id=021895a6-149a-4282-aa8e-6719e7d7c47a&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" loading=lazy alt="Screen Shot 2023-07-01 at 12.25.37 PM.png"></p><blockquote><p>Alt: Picture of Plan & SolveAltï¼šè®¡åˆ’å’Œè§£å†³æ–¹æ¡ˆçš„å›¾ç‰‡</p></blockquote><p>While PS prompting helps evoke a reasoning response, it still misses a fundamental concept in reasoning, and that is proper handling for reflection and action. <strong>Reflection</strong>is <em>fundamental for any agent because it must rationalize an action, perform that action, and use feedback to adjust future actions.</em> Without it, the agent would be stateless and unchanging.
è™½ç„¶ PS æç¤ºæœ‰åŠ©äºå”¤èµ·æ¨ç†ååº”ï¼Œä½†å®ƒä»ç„¶å¿½ç•¥äº†æ¨ç†ä¸­çš„ä¸€ä¸ªåŸºæœ¬æ¦‚å¿µï¼Œé‚£å°±æ˜¯å¯¹åæ€å’Œè¡ŒåŠ¨çš„æ­£ç¡®å¤„ç†ã€‚åæ€å¯¹äºä»»ä½•æ™ºèƒ½ä½“æ¥è¯´éƒ½æ˜¯åŸºç¡€ï¼Œå› ä¸ºå®ƒå¿…é¡»åˆç†åŒ–æŸä¸ªåŠ¨ä½œã€æ‰§è¡Œè¯¥åŠ¨ä½œå¹¶ä½¿ç”¨åé¦ˆæ¥è°ƒæ•´æœªæ¥çš„åŠ¨ä½œã€‚æ²¡æœ‰å®ƒï¼Œä»£ç†å°†æ˜¯æ— çŠ¶æ€ä¸”ä¸å˜çš„ã€‚</p><p>AgentGPT uses a prompting framework called Reasoning and Acting (<a class=link href=https://arxiv.org/pdf/2210.03629.pdf target=_blank rel=noopener>ReAct</a>) to expand on the capabilities of the Plan-and-Solve concept. <strong>ReAct</strong> aims to <em>enable a framework for the model to access fresh knowledge through external knowledge bases and make observations of actions it has taken</em>. Using those observations, the LLM can make educated decisions on the next set of steps to complete while performing actions to query knowledge bases such as <strong>Google Search</strong> or <strong>Wikipedia API</strong>.
AgentGPT ä½¿ç”¨ç§°ä¸ºæ¨ç†å’Œè¡ŒåŠ¨ (ReAct) çš„æç¤ºæ¡†æ¶æ¥æ‰©å±•è®¡åˆ’å’Œè§£å†³æ¦‚å¿µçš„åŠŸèƒ½ã€‚ ReAct æ—¨åœ¨ä¸ºæ¨¡å‹æä¾›ä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿé€šè¿‡å¤–éƒ¨çŸ¥è¯†åº“è·å–æ–°çŸ¥è¯†å¹¶è§‚å¯Ÿå…¶æ‰€é‡‡å–çš„è¡ŒåŠ¨ã€‚åˆ©ç”¨è¿™äº›è§‚å¯Ÿç»“æœï¼Œæ³•å­¦ç¡•å£«å¯ä»¥å¯¹ä¸‹ä¸€ç»„è¦å®Œæˆçš„æ­¥éª¤åšå‡ºæ˜æ™ºçš„å†³ç­–ï¼ŒåŒæ—¶æ‰§è¡ŒæŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆä¾‹å¦‚ Google æœç´¢æˆ–ç»´åŸºç™¾ç§‘ APIï¼‰çš„æ“ä½œã€‚</p><p>Prompt engineering is largely effective in resolving challenges in short-term memory as well as instilling the reasoning behavior that you can see when AgentGPT is at work. However, prompt engineering does not resolve the issue of long-term memory. This issue is where vector databases come in, and we will look at those next.
å³æ—¶å·¥ç¨‹åœ¨è§£å†³çŸ­æœŸè®°å¿†æŒ‘æˆ˜ä»¥åŠçŒè¾“ AgentGPT å·¥ä½œæ—¶å¯ä»¥çœ‹åˆ°çš„æ¨ç†è¡Œä¸ºæ–¹é¢éå¸¸æœ‰æ•ˆã€‚ç„¶è€Œï¼Œå³æ—¶å·¥ç¨‹å¹¶ä¸èƒ½è§£å†³é•¿æœŸè®°å¿†çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜å°±æ˜¯çŸ¢é‡æ•°æ®åº“çš„ç”¨æ­¦ä¹‹åœ°ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°†è®¨è®ºè¿™äº›é—®é¢˜ã€‚</p><p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F481f0812-00e5-4cb1-9ed6-4f2f9215eef5%2FScreen_Shot_2023-07-03_at_3.12.56_AM.png?id=8002f409-2913-4e68-b8b6-6100c4128cf5&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" loading=lazy alt="Screen Shot 2023-07-03 at 3.12.56 AM.png"></p><blockquote><p>Alt : ReAct (Reason + Act) Logic PictureAlt : ReAct (ç†æ€§ + è¡ŒåŠ¨) é€»è¾‘å›¾</p></blockquote><blockquote><p>The ReAct framework allows us to generate a reasoning response, an action, and a reflection to steer the modelâ€™s response. This example is courtesy of the following paper: <a class=link href=https://arxiv.org/abs/2210.03629 target=_blank rel=noopener>ReAct: Synergizing Reasoning and Acting in Language Models</a>*ReAct æ¡†æ¶å…è®¸æˆ‘ä»¬ç”Ÿæˆæ¨ç†å“åº”ã€æ“ä½œå’Œåå°„æ¥å¼•å¯¼æ¨¡å‹çš„å“åº”ã€‚æ­¤ç¤ºä¾‹ç”±ä»¥ä¸‹è®ºæ–‡æä¾›ï¼šReActï¼šåœ¨è¯­è¨€æ¨¡å‹ä¸­ååŒæ¨ç†å’Œè¡ŒåŠ¨*</p></blockquote><h2 id=how-do-we-give-agents-a-working-memoryæˆ‘ä»¬å¦‚ä½•ä¸ºä»£ç†æä¾›å·¥ä½œè®°å¿†>How do we give agents a working memory?æˆ‘ä»¬å¦‚ä½•ä¸ºä»£ç†æä¾›å·¥ä½œè®°å¿†ï¼Ÿ</h2><p>While we have seen that <em>prompt engineering is largely effective in resolving issues with short-term memory and reasoning</em>, we cannot solve long-term memory solely through clever English. Since we are not allowed to update the model to learn our data, we must build an external system for storing and retrieving knowledge.è™½ç„¶æˆ‘ä»¬å·²ç»çœ‹åˆ°å³æ—¶å·¥ç¨‹åœ¨è§£å†³çŸ­æœŸè®°å¿†å’Œæ¨ç†é—®é¢˜æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†æˆ‘ä»¬ä¸èƒ½ä»…é€šè¿‡èªæ˜çš„è‹±è¯­æ¥è§£å†³é•¿æœŸè®°å¿†ã€‚ç”±äºæˆ‘ä»¬ä¸å…è®¸æ›´æ–°æ¨¡å‹æ¥å­¦ä¹ æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ„å»ºä¸€ä¸ªå¤–éƒ¨ç³»ç»Ÿæ¥å­˜å‚¨å’Œæ£€ç´¢çŸ¥è¯†ã€‚</p><p>A clever solution might use an LLM to <em>generate summaries of previous conversations as context for the prompt</em>. However, there are three significant issues with this. First, we are diluting the relevant information for the conversation; second, it introduces another cost area by paying for API usage for those summaries; and third, it&rsquo;s unscalable.ä¸€ä¸ªèªæ˜çš„è§£å†³æ–¹æ¡ˆå¯èƒ½ä¼šä½¿ç”¨æ³•å­¦ç¡•å£«æ¥ç”Ÿæˆä»¥å‰å¯¹è¯çš„æ‘˜è¦ä½œä¸ºæç¤ºçš„ä¸Šä¸‹æ–‡ã€‚ç„¶è€Œï¼Œè¿™å­˜åœ¨ä¸‰ä¸ªé‡å¤§é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ·¡åŒ–å¯¹è¯çš„ç›¸å…³ä¿¡æ¯ï¼›å…¶æ¬¡ï¼Œå®ƒå¼•å…¥äº†å¦ä¸€ä¸ªæˆæœ¬é¢†åŸŸï¼Œå³ä¸ºè¿™äº›æ‘˜è¦çš„ API ä½¿ç”¨ä»˜è´¹ï¼›ç¬¬ä¸‰ï¼Œå®ƒæ˜¯ä¸å¯æ‰©å±•çš„ã€‚</p><p>Thus, prompts appear to be ineffective for long-term memory. Seeing as <em>long-term memory is a problem of storage and efficient retrieval of information</em>, there is no absence of research in the study of search, so we must look towards vector databases.å› æ­¤ï¼Œæç¤ºä¼¼ä¹å¯¹é•¿æœŸè®°å¿†æ— æ•ˆã€‚ç”±äºé•¿æœŸè®°å¿†æ˜¯ä¸€ä¸ªä¿¡æ¯å­˜å‚¨å’Œé«˜æ•ˆæ£€ç´¢çš„é—®é¢˜ï¼Œæœç´¢çš„ç ”ç©¶å¹¶ä¸ç¼ºä¹ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»å°†ç›®å…‰æŠ•å‘å‘é‡æ•°æ®åº“ã€‚</p><h3 id=vector-databases-demystifiedæ­ç§˜çŸ¢é‡æ•°æ®åº“>Vector Databases Demystifiedæ­ç§˜çŸ¢é‡æ•°æ®åº“</h3><p><strong><a class=link href=https://aws.amazon.com/what-is/vector-databases/ target=_blank rel=noopener>Vector databases</a></strong> have been hyped up for a while now, and the hype is very deserved. They are an efficient way of storing and retrieving vectors by allowing us to use some fun new *algorithms to query billions - even trillions - of data records in milliseconds.*çŸ¢é‡æ•°æ®åº“å·²ç»è¢«ç‚’ä½œæœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œè€Œä¸”è¿™ç§ç‚’ä½œæ˜¯éå¸¸å€¼å¾—çš„ã€‚å®ƒä»¬æ˜¯å­˜å‚¨å’Œæ£€ç´¢å‘é‡çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå…è®¸æˆ‘ä»¬ä½¿ç”¨ä¸€äº›æœ‰è¶£çš„æ–°ç®—æ³•åœ¨å‡ æ¯«ç§’å†…æŸ¥è¯¢æ•°åäº¿ç”šè‡³æ•°ä¸‡äº¿æ¡æ•°æ®è®°å½•ã€‚</p><p>Let&rsquo;s start with a little bit of vocabulary:è®©æˆ‘ä»¬ä»ä¸€äº›è¯æ±‡å¼€å§‹ï¼š</p><ul><li>A <strong>vector</strong> in the context of an LLM is a representation of a piece of text that a model like GPT-4 encodes.LLM ä¸Šä¸‹æ–‡ä¸­çš„å‘é‡æ˜¯ GPT-4 ç­‰æ¨¡å‹ç¼–ç çš„ä¸€æ®µæ–‡æœ¬çš„è¡¨ç¤ºã€‚</li><li>A <strong>vector space</strong> contains many of these vectors.å‘é‡ç©ºé—´åŒ…å«è®¸å¤šè¿™æ ·çš„å‘é‡ã€‚</li><li>An <strong>embedding</strong> is the vectorized version of a text.åµŒå…¥æ˜¯æ–‡æœ¬çš„çŸ¢é‡åŒ–ç‰ˆæœ¬ã€‚</li></ul><h3 id=vector-libraries-likeå‘é‡åº“å¦‚>Vector libraries likeÂ å‘é‡åº“å¦‚</h3><p><a class=link href="https://www.bing.com/ck/a?!&&amp;p=a0f4167bc6cd7db9JmltdHM9MTY4ODM0MjQwMCZpZ3VpZD0zOTYwYjczZS1hNzg2LTY5Y2MtMjM2YS1hNDdmYTYwMjY4MjImaW5zaWQ9NTIwMQ&amp;ptn=3&amp;hsh=3&amp;fclid=3960b73e-a786-69cc-236a-a47fa6026822&amp;psq=faiss+github&amp;u=a1aHR0cHM6Ly9naXRodWIuY29tL2ZhY2Vib29rcmVzZWFyY2gvZmFpc3M&amp;ntb=1" target=_blank rel=noopener>Facebook AI Similarity Search</a> ( FAISS) give us access to valuable *tools to control these vectors and locate them efficiently in the vector space.*Facebook AI ç›¸ä¼¼æ€§æœç´¢ (FAISS) ä¸ºæˆ‘ä»¬æä¾›äº†å®è´µçš„å·¥å…·æ¥æ§åˆ¶è¿™äº›å‘é‡å¹¶åœ¨å‘é‡ç©ºé—´ä¸­æœ‰æ•ˆåœ°å®šä½å®ƒä»¬ã€‚</p><p>Since the text is in a numerical embedding dictated by the model type (i.e., text-embedding-ada-002), there is some location in space that the text exists in, and it&rsquo;s based on the numbers that compose its vector. That means <em>similar texts will be represented as vectors with similar numbers, and thus, they will likely be grouped closely. On the other hand, less similar texts will be further away</em>. For example, texts about cooking will be closer to food than texts about physics.ç”±äºæ–‡æœ¬å¤„äºç”±æ¨¡å‹ç±»å‹æŒ‡å®šçš„æ•°å­—åµŒå…¥ä¸­ï¼ˆå³ text-embedding-ada-002ï¼‰ï¼Œå› æ­¤æ–‡æœ¬å­˜åœ¨äºç©ºé—´ä¸­çš„æŸä¸ªä½ç½®ï¼Œå¹¶ä¸”å®ƒåŸºäºç»„æˆå…¶å‘é‡çš„æ•°å­—ã€‚è¿™æ„å‘³ç€ç›¸ä¼¼çš„æ–‡æœ¬å°†è¢«è¡¨ç¤ºä¸ºå…·æœ‰ç›¸ä¼¼æ•°å­—çš„å‘é‡ï¼Œå› æ­¤å®ƒä»¬å¯èƒ½ä¼šè¢«ç´§å¯†åœ°åˆ†ç»„ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸å¤ªç›¸ä¼¼çš„æ–‡æœ¬ä¼šç¦»å¾—æ›´è¿œã€‚ä¾‹å¦‚ï¼Œå…³äºçƒ¹é¥ªçš„æ–‡æœ¬æ¯”å…³äºç‰©ç†çš„æ–‡æœ¬æ›´æ¥è¿‘é£Ÿç‰©ã€‚</p><p>There are several different algorithms for querying the vector space, but the most relevant to this discussion is the cosine similarity search. <strong><a class=link href=https://www.geeksforgeeks.org/cosine-similarity/ target=_blank rel=noopener>Cosine similarity</a></strong> measures the cosine of the angle between two non-zero vectors. <em>It is a measure of orientation, meaning that it&rsquo;s used to determine how similar two documents (or whatever the vectors represent) are</em>. Cosine similarity can range from -1 to 1, with -1 meaning the vectors are diametrically opposed (completely opposite), 0 meaning the vectors are orthogonal (or unrelated), and 1 meaning the vectors are identical.æœ‰å‡ ç§ä¸åŒçš„ç®—æ³•ç”¨äºæŸ¥è¯¢å‘é‡ç©ºé—´ï¼Œä½†ä¸æœ¬è®¨è®ºæœ€ç›¸å…³çš„æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æµ‹é‡ä¸¤ä¸ªéé›¶å‘é‡ä¹‹é—´è§’åº¦çš„ä½™å¼¦ã€‚å®ƒæ˜¯æ–¹å‘çš„åº¦é‡ï¼Œè¿™æ„å‘³ç€å®ƒç”¨äºç¡®å®šä¸¤ä¸ªæ–‡æ¡£ï¼ˆæˆ–å‘é‡è¡¨ç¤ºçš„ä»»ä½•å†…å®¹ï¼‰çš„ç›¸ä¼¼ç¨‹åº¦ã€‚ä½™å¼¦ç›¸ä¼¼åº¦çš„èŒƒå›´ä¸º -1 åˆ° 1ï¼Œå…¶ä¸­ -1 è¡¨ç¤ºå‘é‡å®Œå…¨ç›¸åï¼ˆå®Œå…¨ç›¸åï¼‰ï¼Œ0 è¡¨ç¤ºå‘é‡æ­£äº¤ï¼ˆæˆ–ä¸ç›¸å…³ï¼‰ï¼Œ1 è¡¨ç¤ºå‘é‡ç›¸åŒã€‚</p><p>FAISS is helpful in managing these vector spaces, but it is not a database. <em>Vector libraries lack <a class=link href=https://www.freecodecamp.org/news/crud-operations-explained/ target=_blank rel=noopener>CRUD</a> operations, which makes them alone unviable for long-term memory</em>, and that&rsquo;s where cloud services such as Pinecone and Weaviate step in.FAISS æœ‰åŠ©äºç®¡ç†è¿™äº›å‘é‡ç©ºé—´ï¼Œä½†å®ƒä¸æ˜¯æ•°æ®åº“ã€‚çŸ¢é‡åº“ç¼ºä¹ CRUD æ“ä½œï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•å•ç‹¬ç”¨äºé•¿æœŸè®°å¿†ï¼Œè€Œè¿™æ­£æ˜¯ Pinecone å’Œ Weaviate ç­‰äº‘æœåŠ¡ä»‹å…¥çš„åœ°æ–¹ã€‚</p><p><strong>Pinecone</strong> and <strong>Weaviate</strong> essentially do all the hard work of managing our vectors. They provide an API that allows you to upload embeddings, perform various types of searches, and store those vectors for later. *They provide the typical CRUD functions we need to instill memory into LLMs in easily-accessible Python modules.*Pinecone å’Œ Weaviate åŸºæœ¬ä¸Šå®Œæˆäº†ç®¡ç†æˆ‘ä»¬è½½ä½“çš„æ‰€æœ‰è‰°è‹¦å·¥ä½œã€‚ä»–ä»¬æä¾›äº†ä¸€ä¸ª APIï¼Œå…è®¸æ‚¨ä¸Šä¼ åµŒå…¥ã€æ‰§è¡Œå„ç§ç±»å‹çš„æœç´¢å¹¶å­˜å‚¨è¿™äº›å‘é‡ä»¥ä¾›ä»¥åä½¿ç”¨ã€‚å®ƒä»¬æä¾›äº†æˆ‘ä»¬éœ€è¦çš„å…¸å‹ CRUD å‡½æ•°ï¼Œä»¥ä¾¿å°†å†…å­˜æ³¨å…¥åˆ°æ˜“äºè®¿é—®çš„ Python æ¨¡å—ä¸­çš„ LLM ä¸­ã€‚</p><p>By using them, we can encode large amounts of information for future storage and retrieval. For instance, when the LLM needs extra knowledge to complete a task, we can prompt it to query the vector space to find relevant information. Thus, we can create long-term memory.é€šè¿‡ä½¿ç”¨å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å¤§é‡ä¿¡æ¯è¿›è¡Œç¼–ç ä»¥ä¾›å°†æ¥å­˜å‚¨å’Œæ£€ç´¢ã€‚ä¾‹å¦‚ï¼Œå½“LLMéœ€è¦é¢å¤–çš„çŸ¥è¯†æ¥å®Œæˆä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æç¤ºå®ƒæŸ¥è¯¢å‘é‡ç©ºé—´ä»¥æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›é€ é•¿æœŸè®°å¿†ã€‚</p><p><img src="https://petal-diplodocus-04a.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fad2521b3-1c6b-4f16-b719-d2b766570c61%2FCybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp?id=8d261d10-f4e4-4798-bc33-8f40da67bb42&amp;table=block&amp;spaceId=46c3481b-d8de-4c34-8647-2292d63a5f29&amp;width=2000&amp;userId=&amp;cache=v2" loading=lazy alt=CybrCo_Art_A_human-like_robot_touching_a_flower_for_the_first_t_92e97d56-54fa-4bb0-8581-5a1e15fd94aa.webp></p><blockquote><p>Alt : Robot With A Rose In HandAlt : æ‰‹æ¡ç«ç‘°çš„æœºå™¨äºº</p></blockquote><h2 id=tools-to-interact-with-the-environmentä¸ç¯å¢ƒäº¤äº’çš„å·¥å…·>Tools to interact with the environmentä¸ç¯å¢ƒäº¤äº’çš„å·¥å…·</h2><p>While <strong>prompt engineering</strong> and <strong>vector databases</strong> resolve many of the limitations and challenges of LLMs, there is still the problem of agent interaction. *How can we extend the capabilities of an LLM to interact with the environment outside of text?*è™½ç„¶å³æ—¶å·¥ç¨‹å’ŒçŸ¢é‡æ•°æ®åº“è§£å†³äº†æ³•å­¦ç¡•å£«çš„è®¸å¤šé™åˆ¶å’ŒæŒ‘æˆ˜ï¼Œä½†ä»ç„¶å­˜åœ¨ä»£ç†äº¤äº’çš„é—®é¢˜ã€‚æˆ‘ä»¬å¦‚ä½•æ‰©å±•æ³•å­¦ç¡•å£«ä¸æ–‡æœ¬ä¹‹å¤–çš„ç¯å¢ƒäº¤äº’çš„èƒ½åŠ›ï¼Ÿ</p><p>APIs are the answer. By utilizing APIs, we can give our agents the ability to perform a wide range of actions and access external resources.API å°±æ˜¯ç­”æ¡ˆã€‚é€šè¿‡åˆ©ç”¨ APIï¼Œæˆ‘ä»¬å¯ä»¥è®©æˆ‘ä»¬çš„ä»£ç†èƒ½å¤Ÿæ‰§è¡Œå„ç§æ“ä½œå¹¶è®¿é—®å¤–éƒ¨èµ„æºã€‚</p><p>Here are a few examples:è¿™é‡Œæœ‰ä¸€äº›ä¾‹å­ï¼š</p><ul><li><strong>Google Search API</strong>: Allows agents to search the web and retrieve relevant information.Google Search APIï¼šå…è®¸ä»£ç†æœç´¢ç½‘ç»œå¹¶æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚</li><li><strong>Hugging Face</strong>: Provides access to various NLP models and transformers for tasks such as summarization, translation, sentiment analysis, and more.Hugging Faceï¼šæä¾›å¯¹å„ç§ NLP æ¨¡å‹å’Œè½¬æ¢å™¨çš„è®¿é—®ï¼Œä»¥æ‰§è¡Œæ‘˜è¦ã€ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ã€‚</li><li><strong>Dall-E</strong>: Enables agents to generate images from textual descriptions.Dall-Eï¼šä½¿ä»£ç†èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆå›¾åƒã€‚</li><li><strong>OpenAI&rsquo;s GPT API</strong>: Allows agents to utilize the GPT-4 model for text completion and generation.OpenAI çš„ GPT APIï¼šå…è®¸ä»£ç†åˆ©ç”¨ GPT-4 æ¨¡å‹è¿›è¡Œæ–‡æœ¬å®Œæˆå’Œç”Ÿæˆã€‚</li></ul><p>Using API tools in combination with prompt engineering techniques, we can create prompts that generate predictable function calls and utilize the output of API requests to enhance the agent&rsquo;s capabilities. This enables agents to interact with the environment in a meaningful way beyond text-based interactions.ä½¿ç”¨ API å·¥å…·ä¸æç¤ºå·¥ç¨‹æŠ€æœ¯ç›¸ç»“åˆï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºç”Ÿæˆå¯é¢„æµ‹å‡½æ•°è°ƒç”¨çš„æç¤ºï¼Œå¹¶åˆ©ç”¨ API è¯·æ±‚çš„è¾“å‡ºæ¥å¢å¼ºä»£ç†çš„åŠŸèƒ½ã€‚è¿™ä½¿å¾—ä»£ç†èƒ½å¤Ÿä»¥ä¸€ç§è¶…è¶ŠåŸºäºæ–‡æœ¬çš„äº¤äº’çš„æœ‰æ„ä¹‰çš„æ–¹å¼ä¸ç¯å¢ƒäº¤äº’ã€‚</p><h3 id=engineering-robust-function-callså·¥ç¨‹ç¨³å¥çš„å‡½æ•°è°ƒç”¨>Engineering Robust Function Callså·¥ç¨‹ç¨³å¥çš„å‡½æ•°è°ƒç”¨</h3><p>Again, we can achieve tooling through prompt engineering by <em>representing the tool we want to provide for the model</em> as a <strong>function</strong>. <em>We can then tell the model that this function exists in a prompt, so our program can call it programmatically based on the model&rsquo;s response</em>. First, however, we should examine the main challenges in implementing tool interactions: consistency, context, and format.åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„å·¥å…·è¡¨ç¤ºä¸ºå‡½æ•°æ¥é€šè¿‡å³æ—¶å·¥ç¨‹æ¥å®ç°å·¥å…·ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å‘Šè¯‰æ¨¡å‹è¯¥å‡½æ•°å­˜åœ¨äºæç¤ºä¸­ï¼Œå› æ­¤æˆ‘ä»¬çš„ç¨‹åºå¯ä»¥æ ¹æ®æ¨¡å‹çš„å“åº”ä»¥ç¼–ç¨‹æ–¹å¼è°ƒç”¨å®ƒã€‚ç„¶è€Œï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥æ£€æŸ¥å®ç°å·¥å…·äº¤äº’çš„ä¸»è¦æŒ‘æˆ˜ï¼šä¸€è‡´æ€§ã€ä¸Šä¸‹æ–‡å’Œæ ¼å¼ã€‚</p><p>For example, responses tend to vary among chat completions that use the same prompt. Thus, getting the LLM to issue a function call consistently is challenging. A minor solution may include adjusting the <strong>temperature</strong> of the model (a parameter to control the randomness), but the best solution should leverage an LLM&rsquo;s reasoning abilities. Thus, *we can use the ReAct framework to help the llm understand when to issue function calls.*ä¾‹å¦‚ï¼Œä½¿ç”¨ç›¸åŒæç¤ºçš„èŠå¤©å®Œæˆä¹‹é—´çš„å“åº”å¾€å¾€ä¼šæœ‰æ‰€ä¸åŒã€‚å› æ­¤ï¼Œè®©æ³•å­¦ç¡•å£«ä¸€è‡´åœ°å‘å‡ºå‡½æ•°è°ƒç”¨æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä¸€ä¸ªæ¬¡è¦çš„è§£å†³æ–¹æ¡ˆå¯èƒ½åŒ…æ‹¬è°ƒæ•´æ¨¡å‹çš„æ¸©åº¦ï¼ˆæ§åˆ¶éšæœºæ€§çš„å‚æ•°ï¼‰ï¼Œä½†æœ€å¥½çš„è§£å†³æ–¹æ¡ˆåº”è¯¥åˆ©ç”¨æ³•å­¦ç¡•å£«çš„æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ReAct æ¡†æ¶æ¥å¸®åŠ© llm äº†è§£ä½•æ—¶å‘å‡ºå‡½æ•°è°ƒç”¨ã€‚</p><p>In doing this, we will still run into another major issue. How will the LLMs understand what tools are at their disposal? We could include the available tools in a prompt, but this could significantly increase the number of tokens we would need to send to the model. While this may be fine for an application that runs on a couple of tools, it will increase costs as we add more tools to the system. Thus, *we would use vector databases to help the LLM look up relevant tools it needs.*åœ¨è¿™æ ·åšçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šé‡åˆ°å¦ä¸€ä¸ªé‡å¤§é—®é¢˜ã€‚æ³•å­¦ç¡•å£«å¦‚ä½•äº†è§£ä»–ä»¬å¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·ï¼Ÿæˆ‘ä»¬å¯ä»¥åœ¨æç¤ºä¸­åŒ…å«å¯ç”¨çš„å·¥å…·ï¼Œä½†è¿™å¯èƒ½ä¼šæ˜¾ç€å¢åŠ æˆ‘ä»¬éœ€è¦å‘é€åˆ°æ¨¡å‹çš„ä»¤ç‰Œæ•°é‡ã€‚è™½ç„¶è¿™å¯¹äºåœ¨å¤šä¸ªå·¥å…·ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åºæ¥è¯´å¯èƒ½æ²¡é—®é¢˜ï¼Œä½†éšç€æˆ‘ä»¬å‘ç³»ç»Ÿæ·»åŠ æ›´å¤šå·¥å…·ï¼Œå®ƒä¼šå¢åŠ æˆæœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‘é‡æ•°æ®åº“æ¥å¸®åŠ©æ³•å­¦ç¡•å£«æŸ¥æ‰¾å…¶æ‰€éœ€çš„ç›¸å…³å·¥å…·ã€‚</p><p>Finally, we need to generate function calls in a predictable format. This format should include provisions for the name of the function and the parameters it takes, and it must include delimiters that allow us to parse and execute the response for those parameters programmatically. *For instance, you can prompt the model to only return responses in JSON and then use built-in Python libraries to parse the stringified JSON.*æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä»¥å¯é¢„æµ‹çš„æ ¼å¼ç”Ÿæˆå‡½æ•°è°ƒç”¨ã€‚æ­¤æ ¼å¼åº”åŒ…æ‹¬å‡½æ•°åç§°åŠå…¶é‡‡ç”¨çš„å‚æ•°çš„è§„å®šï¼Œå¹¶ä¸”å¿…é¡»åŒ…æ‹¬å…è®¸æˆ‘ä»¬ä»¥ç¼–ç¨‹æ–¹å¼è§£æå’Œæ‰§è¡Œè¿™äº›å‚æ•°çš„å“åº”çš„åˆ†éš”ç¬¦ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æç¤ºæ¨¡å‹ä»…è¿”å› JSON æ ¼å¼çš„å“åº”ï¼Œç„¶åä½¿ç”¨å†…ç½® Python åº“æ¥è§£æå­—ç¬¦ä¸²åŒ–çš„ JSONã€‚</p><p>Recently, it became even easier to use this type of method as well. In late June, OpenAI released <strong>gpt-4-0613</strong> and <strong>gpt-3.5-turbo-16k-0613</strong> (whew, these names are getting long). They natively support function calls by using a model fine-tuned for JSON to return easy-to-use function calls. You can read more about it <a class=link href=https://platform.openai.com/docs/guides/gpt/function-calling target=_blank rel=noopener>here</a>.æœ€è¿‘ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•ä¹Ÿå˜å¾—æ›´åŠ å®¹æ˜“ã€‚ 6æœˆä¸‹æ—¬ï¼ŒOpenAIå‘å¸ƒäº†gpt-4-0613å’Œgpt-3.5-turbo-16k-0613ï¼ˆå“‡ï¼Œè¿™äº›åå­—è¶Šæ¥è¶Šé•¿äº†ï¼‰ã€‚å®ƒä»¬é€šè¿‡ä½¿ç”¨é’ˆå¯¹ JSON è¿›è¡Œå¾®è°ƒçš„æ¨¡å‹æ¥åŸç”Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼Œä»¥è¿”å›æ˜“äºä½¿ç”¨çš„å‡½æ•°è°ƒç”¨ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œè¯»æ›´å¤šå…³äºå®ƒçš„å†…å®¹ã€‚</p><h2 id=the-future-of-llm-powered-agents-is-brightllm-ä»£ç†äººçš„æœªæ¥æ˜¯å…‰æ˜çš„>The future of LLM-powered agents is bright!LLM ä»£ç†äººçš„æœªæ¥æ˜¯å…‰æ˜çš„ï¼</h2><p>Large language models have been one of the most significant advances of the past decade. Capable of reasoning and talking like a human, they appear to be able to do anything. Despite this, several engineering challenges arise in building around an LLM, such as context limits, reasoning, and long-term retention.å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯è¿‡å»åå¹´æœ€é‡å¤§çš„è¿›æ­¥ä¹‹ä¸€ã€‚å®ƒä»¬èƒ½å¤Ÿåƒäººç±»ä¸€æ ·æ¨ç†å’Œè¯´è¯ï¼Œä¼¼ä¹èƒ½å¤Ÿåšä»»ä½•äº‹æƒ…ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå›´ç»•æ³•å­¦ç¡•å£«çš„æ„å»ºä»ä¼šå‡ºç°ä¸€äº›å·¥ç¨‹æŒ‘æˆ˜ï¼Œä¾‹å¦‚ä¸Šä¸‹æ–‡é™åˆ¶ã€æ¨ç†å’Œé•¿æœŸä¿ç•™ã€‚</p><p>Using the methods described above, <strong>AgentGPT</strong> unlocks the full potential of powerful models such as GPT-4. <em>We can give any model superpowers using novel prompting methods, efficient vector databases, and abundant API tools</em>. It&rsquo;s only the start, and we hope you&rsquo;ll join us on this journey.ä½¿ç”¨ä¸Šè¿°æ–¹æ³•ï¼ŒAgentGPT é‡Šæ”¾äº† GPT-4 ç­‰å¼ºå¤§æ¨¡å‹çš„å…¨éƒ¨æ½œåŠ›ã€‚é€šè¿‡æ–°é¢–çš„æç¤ºæ–¹æ³•ã€é«˜æ•ˆçš„å‘é‡æ•°æ®åº“ã€ä¸°å¯Œçš„APIå·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥èµ‹äºˆä»»ä½•æ¨¡å‹è¶…èƒ½åŠ›ã€‚è¿™åªæ˜¯ä¸€ä¸ªå¼€å§‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ‚¨èƒ½åŠ å…¥æˆ‘ä»¬çš„æ—…ç¨‹ã€‚</p><h2 id=conclusionç»“è®º>ConclusionÂ ç»“è®º</h2><p>AgentGPT represents a powerful approach to building AI agents that reason, remember, and perform. By leveraging prompt engineering, vector databases, and API tools, we can overcome the limitations of standalone LLMs and create agents that demonstrate agentic behavior.AgentGPT ä»£è¡¨äº†æ„å»ºå…·æœ‰æ¨ç†ã€è®°å¿†å’Œæ‰§è¡ŒåŠŸèƒ½çš„ AI ä»£ç†çš„å¼ºå¤§æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å³æ—¶å·¥ç¨‹ã€çŸ¢é‡æ•°æ®åº“å’Œ API å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥å…‹æœç‹¬ç«‹æ³•å­¦ç¡•å£«çš„å±€é™æ€§ï¼Œå¹¶åˆ›å»ºèƒ½å¤Ÿå±•ç¤ºä»£ç†è¡Œä¸ºçš„ä»£ç†ã€‚</p><p>With the ability to reason, plan, and reflect, AgentGPT agents can tackle complex tasks and interact with the environment in a meaningful way. By incorporating long-term memory through vector databases and utilizing APIs, we provide agents with access to a vast pool of knowledge and resources.å‡­å€Ÿæ¨ç†ã€è®¡åˆ’å’Œåæ€çš„èƒ½åŠ›ï¼ŒAgentGPT ä»£ç†å¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡å¹¶ä»¥æœ‰æ„ä¹‰çš„æ–¹å¼ä¸ç¯å¢ƒäº¤äº’ã€‚é€šè¿‡å‘é‡æ•°æ®åº“æ•´åˆé•¿æœŸè®°å¿†å¹¶åˆ©ç”¨ APIï¼Œæˆ‘ä»¬ä¸ºä»£ç†æä¾›äº†è®¿é—®å¤§é‡çŸ¥è¯†å’Œèµ„æºçš„æœºä¼šã€‚</p><p>AgentGPT is a step towards unlocking the full potential of LLMs and creating intelligent agents that can assist and collaborate with humans in various domains. The combination of language models, prompt engineering, external memory, and API interactions opens up exciting possibilities for AI agents in the future.AgentGPT æ˜¯æœç€é‡Šæ”¾æ³•å­¦ç¡•å£«çš„å…¨éƒ¨æ½œåŠ›å’Œåˆ›å»ºå¯ä»¥åœ¨å„ä¸ªé¢†åŸŸååŠ©äººç±»å¹¶ä¸äººç±»åä½œçš„æ™ºèƒ½ä»£ç†è¿ˆå‡ºçš„ä¸€æ­¥ã€‚è¯­è¨€æ¨¡å‹ã€å³æ—¶å·¥ç¨‹ã€å¤–éƒ¨å­˜å‚¨å™¨å’Œ API äº¤äº’çš„ç»“åˆä¸ºäººå·¥æ™ºèƒ½ä»£ç†çš„æœªæ¥å¼€è¾Ÿäº†ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ã€‚</p></section><footer class=article-footer><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg><span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/xagent-video-%E5%A4%A7%E7%BA%B2/><div class=article-details><h2 class=article-title>Xagent Video å¤§çº²</h2></div></a></article><article class=has-image><a href=/p/agentverse/><div class=article-image><img src=/p/agentverse/cover.c0a185479fc7a02ece872a95b555cf71_hu562ff14b3d00ed1bfd5a51b32681b45e_1477547_250x150_fill_box_smart1_3.png width=250 height=150 loading=lazy alt="Featured image of post Agentverse" data-hash="md5-wKGFR5/HoC7OhyqVtVXPcQ=="></div><div class=article-details><h2 class=article-title>Agentverse</h2></div></a></article></div></div></aside><div class=disqus-container><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//Arno.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2023 Arno's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.21.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>